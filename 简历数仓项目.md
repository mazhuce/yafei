- **项目名称: 基于Iceberg的电商业务流批一体实时数仓**

- **技术栈**: `Flink , Apache Iceberg`, `Spark`, `Kafka`, `dbt`, `Trino`, `Airflow`, `HDFS
    
- **项目描述**: 设计湖仓一体数据平台，打通了实时与离线数据链路，实现了核心业务指标的近实时分析，赋能业务精细化运营。

- **项目设计**: 设计了以 Apache Iceberg 为核心存储底座的湖仓架构。使用 Flink CDC 实时捕获MySQL 业务库的 binlog ，结合用户行为日志流，通过Flink SQL将数据实时写入Iceberg数据湖的ODS层和DWD层，实现了数据分钟级入湖。使用 Flink 对DWD层数据进行轻度聚合，计算实时DWM层，结果直接写入Doris提供实时查询。使用 Spark 运行复杂批量任务，计算DWS和ADS层的汇总数据。引入 dbt ，在 Iceberg 数据湖之上构建可维护、可测试的SQL数据模型。通过dbt实现数据转换、文档生成和数据质量测试，确保数据口径的统一和准确性。部署 Trino 查询引擎，提供对 Iceberg 数据湖的统一、高性能的Ad-hoc即席查询入口。


- **项目名称**: **基于Netty与Disruptor的高性能实时日志采集与预处理网关**
    
- **技术栈**: `Java`, `Netty`, `Disruptor`, `Protobuf`, `Kafka`, `JCTools`, `Prometheus`
    
- **项目描述**: 为解决前端业务系统日志上报的性能瓶颈，设计并实现了一个高吞吐、低延迟的实时数据网关，作为整个数据平台的数据入口，为下游的湖仓提供稳定、标准的数据源。
    
- **项目设计**:  采用 Netty 框架，利用其NIO和零拷贝特性实现高并发网络连接。自定义 Protobuf 编解码器，提升序列化性能和网络传输效率。引入 LMAX Disruptor 作为核心内存队列，构建了无锁并发模型，解耦网络I/O线程和业务处理线程，极大地减少了线程上下文切换和锁竞争开销。设计了多阶段处理流程，包括数据校验、业务字段提取、敏感信息脱敏、基于IP的地理位置信息补全等。利用 JCTools 中的高性能并发集合进行黑名单IP过滤，并将处理完成的标准化数据批量、异步地发送到 Kafka 消息队列。集成 Prometheus 和 Grafana，对网关的QPS、延迟、队列积压、GC等关键指标进行实时监控和告警，据此进行JVM参数和线程池调优。


- **项目名称:  面向业务的机器学习MaaS平台***

- **技术栈**: `Spring Boot`, `Vue.js`,`Spark MLlib`,`Docker`, `Kubernetes`, `MLflow`, ,`Redis`

- **项目描述**:  构建集模型训练、管理、部署、服务于一体的MaaS平台，实现了算法模型的一键部署与服务化。

- **架构设计**: 基于 Spring Boot 开发后端服务，负责用户认证、项目管理、模型元数据管理，并通过API驱动整个ML生命周期。使用 Vue.js 和 Element UI 开发前端界面，提供模型上传、训练参数配置、实验结果查看等可视化操作。利用 Spark MLlib 作为后端分布式训练引擎，平台通过API调用触发 Airflow DAG 来执行Spark训练任务。集成 MLflow Tracking，自动记录模型训练的参数、指标和产物。利用 Kubernetes 和 Docker，根据包含模型代码和依赖描述的文件，构建Docker镜像，实现模型服务一键部署。平台根据发布请求自动拉取对应版本的模型镜像，创建Deployment和Service，生成可供业务方调用的RESTful API。


- **项目名称:  基于 DataHub 的企业级一站式数据资产与治理平台**

- **技术栈**: `DataHub`, `Neo4j`, `Elasticsearch`, `Great Expectations`, `Spark SQL`, `Python`, `Airflow`

- **项目描述**:  基于开源项目DataHub进行二次开发，构建了集数据发现、血缘追踪、质量监控于一体的数据治理平台。

- **项目设计**: 部署并改造开源 DataHub 平台，编写自定义 Ingestion Source，采集 Trino、Doris、Airflow 等内部系统的元数据，实现数据库、湖表、BI 报表和任务流的统一管理。分析 Spark SQL 和 Flink SQL 执行计划，自动抽取表级和字段级血缘关系，存入 Neo4j 图数据库，并在前端实现可视化追溯。集成 Great Expectations，设计 YAML 格式的声明式质量规则，通过 Airflow 调度 Spark 任务每日批量校验并生成质量报告。利用 Elasticsearch 为全量元数据建立多维度检索能力，将血缘关系与数据质量评分集成到搜索结果，方便业务用户精准发现并评估数据。